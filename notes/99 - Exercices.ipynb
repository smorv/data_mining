{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice: **\n",
    "\n",
    "Take countries_GDP.csv and convert it to a valid csv file without header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_code position    country_name           gdp\n",
      "0          USA        1   United States   17,946,996 \n",
      "1          CHN        2           China   10,866,444 \n",
      "2          JPN        3           Japan    4,123,258 \n",
      "3          DEU        4         Germany    3,355,772 \n",
      "4          GBR        5  United Kingdom    2,848,755 \n",
      "country_code    object\n",
      "position        object\n",
      "country_name    object\n",
      "gdp             object\n",
      "dtype: object\n",
      "float64\n",
      "  country_code position    country_name         gdp\n",
      "0          USA        1   United States  17946996.0\n",
      "1          CHN        2           China  10866444.0\n",
      "2          JPN        3           Japan   4123258.0\n",
      "3          DEU        4         Germany   3355772.0\n",
      "4          GBR        5  United Kingdom   2848755.0\n",
      "    country_code position         country_name         gdp\n",
      "224          SSF      NaN   Sub-Saharan Africa   1572873.0\n",
      "225          LIC      NaN           Low income    392904.0\n",
      "226          LMC      NaN  Lower middle income   5820363.0\n",
      "227          UMC      NaN  Upper middle income  19732884.0\n",
      "228          HIC      NaN          High income  46985247.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#read csv as data frame\n",
    "df_gdp_raw = pd.read_csv(\"../data/countries_GDP.csv\")\n",
    "#select columns and use these that have data in 'Unamed:0', which\n",
    "#actually is the country code\n",
    "df_gdp = df_gdp_raw[[0,1,3,4]][df_gdp_raw['Unnamed: 0'].notnull()]\n",
    "#rename columns and index\n",
    "df_gdp.columns=[\"country_code\",\"position\",\"country_name\",\"gdp\"]\n",
    "df_gdp.index = range(df_gdp.shape[0])\n",
    "#show head\n",
    "print df_gdp.head()\n",
    "#show types, take into account that gdp should be integer\n",
    "print df_gdp.dtypes\n",
    "#change gdp dtype to numeric\n",
    "df_gdp.gdp = df_gdp.gdp.apply(lambda x: x.replace(\",\",\"\").strip(\" \"))\n",
    "df_gdp.gdp = pd.to_numeric(df_gdp.gdp,errors=\"coerce\")\n",
    "print df_gdp.gdp.dtype\n",
    "print df_gdp.head()\n",
    "print df_gdp.tail()\n",
    "#save as csv, set header as false\n",
    "df_gdp.to_csv(\"../data/countries_GDP_clean.csv\",header=False,sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:**\n",
    "\n",
    "Clean countries data and save it as a valid csv without header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_country_raw = pd.read_csv(\"../data/countries_data.csv\",sep=\";\")\n",
    "df_country_raw.head(15)\n",
    "df_country_raw.to_csv(\"../data/countries_data_clean.csv\",header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:**\n",
    "\n",
    "Build a function that generates a dataframe with N user id plus a list of a random number of random news topics from news_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>world|healthcare|comedians|directors</td>\n",
       "      <td>user0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economists|studios|video games</td>\n",
       "      <td>user1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>automotive|food|movies</td>\n",
       "      <td>user2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reporters</td>\n",
       "      <td>user3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us|video games|environment|banks|basketball</td>\n",
       "      <td>user4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>books|ncaa|theatre</td>\n",
       "      <td>user5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grammys|horses|sportscasters|immigration|children</td>\n",
       "      <td>user6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>elections|banks|immigration</td>\n",
       "      <td>user7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>energy|artists</td>\n",
       "      <td>user8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>figure skating|energy</td>\n",
       "      <td>user9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              topics username\n",
       "0               world|healthcare|comedians|directors    user0\n",
       "1                     economists|studios|video games    user1\n",
       "2                             automotive|food|movies    user2\n",
       "3                                          reporters    user3\n",
       "4        us|video games|environment|banks|basketball    user4\n",
       "5                                 books|ncaa|theatre    user5\n",
       "6  grammys|horses|sportscasters|immigration|children    user6\n",
       "7                        elections|banks|immigration    user7\n",
       "8                                     energy|artists    user8\n",
       "9                              figure skating|energy    user9"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_users_df(num_users, num_topics):\n",
    "    #generate num_users usernames\n",
    "    usernames_df = pd.Series([\"user\"]*num_users).str.cat(pd.Series(np.arange(num_users)).map(str))\n",
    "\n",
    "    #read topics csv\n",
    "    news_topics = pd.read_csv(\"../data/news_topics.csv\",header=None)\n",
    "    #generate a list of N int picked uniformly random from range 0 .. num_topics\n",
    "    #WARNING: is really an uniform distribution??\n",
    "    rand_ints = pd.Series(np.random.randint(1,num_topics+1,num_users))\n",
    "\n",
    "    #WARNING: what happens if x>len(news_topics)\n",
    "    topics_df = rand_ints.apply(lambda x: \"|\".join(np.random.choice(news_topics.T[0],x,replace=False)))\n",
    "\n",
    "    return pd.concat({'username':usernames_df,'topics':topics_df},axis=1)\n",
    "    \n",
    "M = 5\n",
    "N = 100\n",
    "users_df = generate_users_df(N,M)\n",
    "users_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice: ** \n",
    "\n",
    "Save the info generated with the previous function as csv so that it can be easily loaded as a Pair RDD in pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "M = 20\n",
    "N = 1000\n",
    "users_df = generate_users_df(N,M)\n",
    "\n",
    "users_df.to_csv(\"../data/users_events_example/user_info_%susers_%stopics.csv\" % (N,M),\n",
    "                columns=[\"username\",\"topics\"],\n",
    "                header=None, \n",
    "                index=None)\n",
    "                #quoting=csv.QUOTE_MINIMAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice: ** \n",
    "\n",
    "Build a function that generates N csv files containing user's web browsing information. This function takes a max number of users M (from user0 to userM) and generates K user information logs for a randomly picked user (with repetition). The function will return this information with a timestamp. Each file represents 5 minute activity, the activity period will be K/300. The activity information is a random selection of 1 element over news topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  1  of  10  at  2016-01-01 00:00:00\n",
      "File  2  of  10  at  2016-01-01 00:05:00\n",
      "File  3  of  10  at  2016-01-01 00:10:00\n",
      "File  4  of  10  at  2016-01-01 00:15:00\n",
      "File  5  of  10  at  2016-01-01 00:20:00\n",
      "File  6  of  10  at  2016-01-01 00:25:00\n",
      "File  7  of  10  at  2016-01-01 00:30:00\n",
      "File  8  of  10  at  2016-01-01 00:35:00\n",
      "File  9  of  10  at  2016-01-01 00:40:00\n",
      "File  10  of  10  at  2016-01-01 00:45:00\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def generate_user_events(date_start, num_files, num_users, num_events):\n",
    "    #generate usernames\n",
    "    usernames_df = pd.Series([\"user\"]*num_users).str.cat(pd.Series(np.arange(num_users)).map(str))\n",
    "    #read topics\n",
    "    news_topics = pd.read_csv(\"../data/news_topics.csv\",header=None,lineterminator=\"\\n\").T\n",
    "    #create time index\n",
    "    df_index = pd.date_range(date_start, \n",
    "                             periods=num_events, \n",
    "                             freq=pd.DateOffset(seconds=float(5*60)/num_events))\n",
    "    #generate data\n",
    "    event_data = {\"user\" : np.random.choice(usernames_df,num_events,replace=True),\n",
    "                  \"event\" : np.random.choice(news_topics[0],num_events,replace=True)}\n",
    "    #generate df\n",
    "    return pd.DataFrame(event_data, index = df_index, columns=[\"user\", \"event\"])\n",
    "\n",
    "num_files = 10\n",
    "num_users = 100\n",
    "num_events = 1000\n",
    "date_start = datetime.datetime.strptime('1/1/2016', '%d/%m/%Y')\n",
    "\n",
    "for idx,i in enumerate(range(num_files)):\n",
    "    print \"File \",idx+1,\" of \", num_files, \" at \",date_start\n",
    "    userevent_df = generate_user_events(date_start, num_files, num_users, num_events)\n",
    "    file_name = \"../data/users_events_example/userevents_\" + date_start.strftime(\"%d%m%Y%H%M%S\") + \".log\"\n",
    "    userevent_df.to_csv(file_name, header=None)\n",
    "    date_start = date_start + datetime.timedelta(0,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice: **\n",
    "\n",
    "Generate a unique id for papers.lst file and save it as a csv file. Then generate a csv file with random references among papers from papers.lst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "f = file(\"../data/papers.lst\",\"rb\")\n",
    "papers = []\n",
    "for idx,l in enumerate(f.readlines()):\n",
    "    t = re.match(\"(\\d+)(\\s*)(.\\d*)(\\s*)(\\w+)(\\s*)(.*)\",l)\n",
    "    if t:\n",
    "        #print \"|\",t.group(1),\"|\",t.group(3),\"|\",t.group(5),\"|\",t.group(7),\"|\"\n",
    "        papers.append([t.group(1),t.group(3),t.group(5),t.group(7)])\n",
    "            \n",
    "papers_df = pd.DataFrame(papers)\n",
    "papers_df.to_csv(\"../data/papers.csv\", header = None)\n",
    "\n",
    "N = papers_df.shape[0]\n",
    "#let's assume that a paper can have 30 references at max and 5 at min\n",
    "M = 30\n",
    "\n",
    "papers_references = pd.DataFrame(np.arange(N))\n",
    "papers_references[1] = papers_references[0].apply(\n",
    "    lambda x: \n",
    "        \";\".join(\n",
    "            [str(x) for x in np.random.choice(papers_references[0],np.random.randint(5,M))]))\n",
    "papers_references.columns = [\"paper_id\",\"references\"]\n",
    "papers_references.to_csv(\"../data/papers_references.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice: **\n",
    "\n",
    "Read \"../data/country_info_worldbank.xls\" and delete wrong rows and set propper column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cc_df0 = pd.read_excel(\"../data/country_info_worldbank.xls\")\n",
    "#delete unnececary rows\n",
    "cc_df1 = cc_df0[cc_df0[\"Unnamed: 2\"].notnull()]\n",
    "#get columnames and set to dataframe\n",
    "colnames = cc_df1.iloc[0].tolist()\n",
    "colnames[0] = \"Order\"\n",
    "cc_df1.columns = colnames\n",
    "#delete void columns\n",
    "cc_df2 = cc_df1.loc[:,cc_df1.iloc[1].notnull()]\n",
    "#delete first row as it is colnames\n",
    "cc_df3 = cc_df2.iloc[1:]\n",
    "#reindex\n",
    "cc_df3.index = np.arange(cc_df3.shape[0])\n",
    "cc_df3[:][\"Economy\"] = cc_df3.Economy.str.encode('utf-8')\n",
    "cc_df3.to_csv(\"../data/worldbank_countrycodes_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Exercice: **\n",
    "\n",
    "Convert lat lon to UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "est_df = pd.read_csv(\"../data/estacions_meteo.tsv\",sep=\"\\t\")\n",
    "est_df.head()\n",
    "est_df.columns = est_df.columns.str.lower().\\\n",
    "                    str.replace(\"\\[codi\\]\",\"\").\\\n",
    "                    str.replace(\"\\(m\\)\",\"\").str.strip()\n",
    "est_df.longitud = est_df.longitud.str.replace(\",\",\".\")\n",
    "est_df.latitud = est_df.latitud.str.replace(\",\",\".\")\n",
    "\n",
    "est_df.longitud = pd.to_numeric(est_df.longitud)\n",
    "est_df.latitud = pd.to_numeric(est_df.latitud)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice: **\n",
    "\n",
    "Convert to Technically Correct Data: iqsize.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28    132'0\n",
      "34      NaN\n",
      "Name: piq, dtype: object\n",
      "16    73'5\n",
      "26    63'0\n",
      "36     NaN\n",
      "Name: height, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/iqsize.csv\", na_values=\"n/a\")\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "#clean piq\n",
    "errors = pd.to_numeric(df.piq, errors=\"coerce\")\n",
    "print df[\"piq\"][errors.isnull()]\n",
    "df[\"piq\"] = pd.to_numeric(df[\"piq\"].str.replace(\"'\",\".\"))\n",
    "df.dtypes\n",
    "\n",
    "errors = pd.to_numeric(df.height, errors=\"coerce\")\n",
    "print df[\"height\"][errors.isnull()]\n",
    "df[\"height\"] = pd.to_numeric(df[\"height\"].str.replace(\"'\",\".\"))\n",
    "df.dtypes\n",
    "\n",
    "df.sex.unique()\n",
    "df.sex = df.sex.str.replace(\"Woman\",\"Female\")\n",
    "df.sex = df.sex.str.replace(\"woman\",\"Female\")\n",
    "df.sex = df.sex.str.replace(\"woman\",\"Female\")\n",
    "df.sex = df.sex.str.replace(\"man\",\"Male\")\n",
    "df.sex = df.sex.str.replace(\"Man\",\"Male\")\n",
    "df.sex.unique()\n",
    "\n",
    "df.to_csv(\"../data/iqsize_clean.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>piq</th>\n",
       "      <th>brain</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>81.69</td>\n",
       "      <td>64.5</td>\n",
       "      <td>118</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>103.84</td>\n",
       "      <td>73.3</td>\n",
       "      <td>143</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>96.54</td>\n",
       "      <td>68.8</td>\n",
       "      <td>172</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>134.0</td>\n",
       "      <td>95.15</td>\n",
       "      <td>65.0</td>\n",
       "      <td>147</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.88</td>\n",
       "      <td>69.0</td>\n",
       "      <td>146</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>131.0</td>\n",
       "      <td>99.13</td>\n",
       "      <td>64.5</td>\n",
       "      <td>138</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>98.0</td>\n",
       "      <td>85.43</td>\n",
       "      <td>66.0</td>\n",
       "      <td>175</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>84.0</td>\n",
       "      <td>90.49</td>\n",
       "      <td>66.3</td>\n",
       "      <td>134</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>147.0</td>\n",
       "      <td>95.55</td>\n",
       "      <td>68.8</td>\n",
       "      <td>172</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>124.0</td>\n",
       "      <td>83.39</td>\n",
       "      <td>64.5</td>\n",
       "      <td>118</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>128.0</td>\n",
       "      <td>107.95</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>124.0</td>\n",
       "      <td>92.41</td>\n",
       "      <td>69.0</td>\n",
       "      <td>155</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>147.0</td>\n",
       "      <td>85.65</td>\n",
       "      <td>70.5</td>\n",
       "      <td>155</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>90.0</td>\n",
       "      <td>87.89</td>\n",
       "      <td>66.0</td>\n",
       "      <td>146</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>96.0</td>\n",
       "      <td>86.54</td>\n",
       "      <td>68.0</td>\n",
       "      <td>135</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>120.0</td>\n",
       "      <td>85.22</td>\n",
       "      <td>68.5</td>\n",
       "      <td>127</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>102.0</td>\n",
       "      <td>94.51</td>\n",
       "      <td>73.5</td>\n",
       "      <td>178</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>84.0</td>\n",
       "      <td>80.80</td>\n",
       "      <td>66.3</td>\n",
       "      <td>136</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>86.1</td>\n",
       "      <td>88.91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>180</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>84.0</td>\n",
       "      <td>90.59</td>\n",
       "      <td>76.5</td>\n",
       "      <td>186</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>134.0</td>\n",
       "      <td>79.06</td>\n",
       "      <td>62.0</td>\n",
       "      <td>122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>128.0</td>\n",
       "      <td>95.50</td>\n",
       "      <td>68.0</td>\n",
       "      <td>132</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-83.18</td>\n",
       "      <td>63.0</td>\n",
       "      <td>114</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>131.0</td>\n",
       "      <td>93.55</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>171</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>84.0</td>\n",
       "      <td>79.86</td>\n",
       "      <td>68.0</td>\n",
       "      <td>140</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>110.0</td>\n",
       "      <td>106.25</td>\n",
       "      <td>77.0</td>\n",
       "      <td>187</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>72.0</td>\n",
       "      <td>79.35</td>\n",
       "      <td>63.0</td>\n",
       "      <td>106</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>124.0</td>\n",
       "      <td>86.67</td>\n",
       "      <td>66.5</td>\n",
       "      <td>159</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>132.0</td>\n",
       "      <td>85.78</td>\n",
       "      <td>62.5</td>\n",
       "      <td>127</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>137.0</td>\n",
       "      <td>94.96</td>\n",
       "      <td>67.0</td>\n",
       "      <td>191</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>110.0</td>\n",
       "      <td>99.79</td>\n",
       "      <td>75.5</td>\n",
       "      <td>192</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88.00</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>81.0</td>\n",
       "      <td>83.43</td>\n",
       "      <td>66.5</td>\n",
       "      <td>143</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>128.0</td>\n",
       "      <td>94.81</td>\n",
       "      <td>66.5</td>\n",
       "      <td>153</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.94</td>\n",
       "      <td>70.5</td>\n",
       "      <td>144</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.40</td>\n",
       "      <td>64.5</td>\n",
       "      <td>139</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>74.0</td>\n",
       "      <td>93.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>89.0</td>\n",
       "      <td>93.59</td>\n",
       "      <td>75.5</td>\n",
       "      <td>179</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    piq   brain  height  weight     sex\n",
       "0    0  124.0   81.69    64.5     118  Female\n",
       "1    1  150.0  103.84    73.3     143    Male\n",
       "2    2  128.0   96.54    68.8     172  Female\n",
       "3    3  134.0   95.15    65.0     147    Male\n",
       "4    4  110.0   92.88    69.0     146    Male\n",
       "5    5  131.0   99.13    64.5     138    Male\n",
       "6    6   98.0   85.43    66.0     175  Female\n",
       "7    7   84.0   90.49    66.3     134    Male\n",
       "8    8  147.0   95.55    68.8     172  Female\n",
       "9    9  124.0   83.39    64.5     118    Male\n",
       "10  10  128.0  107.95    70.0     151  Female\n",
       "11  11  124.0   92.41    69.0     155    Male\n",
       "12  12  147.0   85.65    70.5     155  Female\n",
       "13  13   90.0   87.89    66.0     146  Female\n",
       "14  14   96.0   86.54    68.0     135    Male\n",
       "15  15  120.0   85.22    68.5     127    Male\n",
       "16  16  102.0   94.51    73.5     178  Female\n",
       "17  17   84.0   80.80    66.3     136  Female\n",
       "18  18   86.1   88.91    70.0     180  Female\n",
       "19  19   84.0   90.59    76.5     186  Female\n",
       "20  20  134.0   79.06    62.0     122     NaN\n",
       "21  21  128.0   95.50    68.0     132    Male\n",
       "22  22  102.0  -83.18    63.0     114  Female\n",
       "23  23  131.0   93.55   -72.0     171    Male\n",
       "24  24   84.0   79.86    68.0     140  Female\n",
       "25  25  110.0  106.25    77.0     187  Female\n",
       "26  26   72.0   79.35    63.0     106  Female\n",
       "27  27  124.0   86.67    66.5     159    Male\n",
       "28  28  132.0   85.78    62.5     127  Female\n",
       "29  29  137.0   94.96    67.0     191    Male\n",
       "30  30  110.0   99.79    75.5     192  Female\n",
       "31  30   86.0   88.00    69.0       0    Male\n",
       "32  32   81.0   83.43    66.5     143  Female\n",
       "33  33  128.0   94.81    66.5     153    Male\n",
       "34  34    NaN   94.94    70.5     144  Female\n",
       "35  35   94.0   89.40    64.5     139    Male\n",
       "36  36   74.0   93.00     NaN     148  Female\n",
       "37  37   89.0   93.59    75.5     179  Female"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/iqsize_clean.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
