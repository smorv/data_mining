{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Apache Spark?\n",
    "* distributed framework\n",
    "* in-memory data structures \n",
    "* data processing\n",
    "* it improves (most of the times) Hadoop workloads\n",
    "\n",
    "Spark enables data scientists to tackle problems with larger data sizes than they could before with tools like R or Pandas\n",
    "\n",
    "## First Steps with Apache Spark Interactive Programming\n",
    "\n",
    "First of all check that PySpark is running properly. You can check if PySpark is correctly loaded:\n",
    "In case it is not, you can follow these posts:\n",
    "    * Windows (IPython): http://jmdvinodjmd.blogspot.com.es/2015/08/installing-ipython-notebook-with-apache.html \n",
    "    * Windows (Jupyter): http://www.ithinkcloud.com/tutorials/tutorial-on-how-to-install-apache-spark-on-windows/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x3b737b8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that with Spark all computation is parallelized by means of distributed data structures that are spread through the cluster. These collections are called Resilient Distributed Datasets (RDD). We will talk more about RDD, as they are the main piece in Spark.\n",
    "\n",
    "As we have successfully loaded the Spark Context, we are ready to do some interactive analysis. We can read a simple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple first example, where we create an RDD (variable lines) and then we apply some operations (count and first) in a parallel manner. It has to be noted, that as we are running all our examples in a single computer the parallelization is not applied. \n",
    "\n",
    "In the next section we will cover the core Spark concepts that allow Spark users to do parallel computation.\n",
    "\n",
    "## Core Spark Concepts\n",
    "\n",
    "We will talk about **Spark applications** that are in charge of loading data and applying some distributed computation over it. Every application has a **driver program** that launches parallel operations to the cluster. In the case of interactive programming, the driver program is the shell (or Notebook) itself.\n",
    "\n",
    "The \"access point\" to Spark from the driver program is the Spark Context object. As we have previously seen, using the referenced documentation, the sc object, is automatically loaded in the notebook.\n",
    "\n",
    "Once we have an Spark Context we can use it to build RDDs. In the previous examples we used sc.textFile() to represent the lines of the textFile. Then we run different operations over the RDD lines. \n",
    "\n",
    "To run these operations over RDDs, driver programs manage different nodes called executors. For example, for the count operation, it is possible to run count in different ranges of the file. \n",
    "\n",
    "Spark's API allows passing functions to its operators to run them on the cluster. For example, we could extend our example by filtering the lines in the file that contain a word, such as individuum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'individuum 1, 42, female, 52.9, brown, 36.9'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "filtered_lines = lines.filter(lambda line: \"individuum\" in line)\n",
    "filtered_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Basics\n",
    "\n",
    "An RDD can be defined as a distributed collection of elements. All work done with Spark can be summarized as **creating**, **transforming** and **applying** operations over RDDs to compute a result. Under the hood, Spark automatically **distributes the data contained in RDDs** across your cluster and **parallelizes the operations** you perform on them.\n",
    "\n",
    "RDD properties:\n",
    "* it is an **immutable distributed** collection of objects\n",
    "* it is split into multiple **partitions**\n",
    "* it is computed on different nodes of the cluster\n",
    "* it can contain any type of Python object (user defined ones included)\n",
    "\n",
    "An RDD can be created in **two ways**:\n",
    "1. loading an external dataset\n",
    "2. distributing a collection of objects in the driver program\n",
    "\n",
    "We have already seen the two ways of creating an RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading an external dataset\n",
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "print type(lines)\n",
    "# applying a transformation to an existing RDD\n",
    "filtered_lines = lines.filter(lambda line: \"individuum\" in line)\n",
    "print type(filtered_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that once we have an RDD, we can run **two kind of operations**:\n",
    "* **transformations**: construct a new RDD from a previous one. For example, by filtering lines RDD we create a new RDD that holds the lines that contain \"individuum\" string. Note that the returning result is an RDD.\n",
    "* **actions**: *compute* a result based on an RDD, and returns the result to the driver program or stores it to an external storage system (e.g. HDFS). Note that the returning result is not an RDD but another kind of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u',Age[years],Sex,Weight[kg],Eye Color,Body Temperature[C]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_result = lines.first()\n",
    "print type(action_result)\n",
    "action_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations and actions are very different because of the way Spark computes RDDs. \n",
    "\n",
    "Transformations are defined in a **lazy** manner this is they are **only computed once they are used in an action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filtered_lines is not computed until the next action is applied over it\n",
    "# it make sense when working with big data sets, as it is not necessary to \n",
    "# transform the whole RDD to get an action over a subset\n",
    "# Spark doesn't even reads the complete file!\n",
    "filtered_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback is that Spark  **recomputes** again the RDD at **each action application**. \n",
    "\n",
    "This means that the computing effort over an already computed RDD may be lost. \n",
    "\n",
    "To mitigate this drawback, the user can take the decision of **persisting** the RDD after computing it the first time, **Spark will store the RDD contents in memory**  (partitioned across the machines in your cluster), and reuse them in future actions. \n",
    "\n",
    "**Persisting RDDs on disk** instead of memory is also possible.\n",
    "\n",
    "Let's see an example on the impact of persisting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count 1: 17.3249998093\n",
      "Word count 2: 16.6799998283\n",
      "Word count persisted 1: 30.1779999733\n",
      "Word count persisted 2: 14.0679998398\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "lines = sc.textFile(\"../data/REFERENCE/*\")\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "words = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words_persisted = lines_nonempty.flatMap(lambda x: x.split())\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 2:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.persist()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 2:\", time.time() - t1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "\n",
    "We have already seen that RDDs have two basic operations: **transformations** and **actions**.\n",
    "\n",
    "**Transformations** are operations that return a new RDD. *Examples:* filter, map.\n",
    "\n",
    "Remember that , transformed RDDs are **computed lazily**, only when you use them in an action.\n",
    "\n",
    "Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), the operation is **not immediately performed**. \n",
    "\n",
    "Instead, Spark internally records **metadata** to indicate that this operation has been requested. \n",
    "\n",
    "**Loading data** into an RDD is lazily evaluated in the same way trans formations are. So, when we call sc.textFile(), the data is **not loaded** until it is necessary. \n",
    "\n",
    "As with transformations, the operation (in this case, reading the data) can occur multiple times. Take in mind that transformations **DO HAVE** impact over computation time.\n",
    "\n",
    "Many transformations are **element-wise**; that is, they work on one element at a time; but this is not true for all transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Newsgroups:',\n",
       " u'freenet.shrine.songs',\n",
       " u'From:',\n",
       " u'aa300',\n",
       " u'(Jerry',\n",
       " u'Murphy)',\n",
       " u'Subject:',\n",
       " u'Songs',\n",
       " u'from',\n",
       " u'the']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"../data/REFERENCE/*\")\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "words = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words_persisted = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter applies the lambda function to each line in lines RDD, only lines that accomplish the condition that the length is greater than zero are in lines_nonempty variable (**this RDD is not computed yet!**)\n",
    "* flatMap applies the lambda function to each element of the RDD and then the result is flattened (i.e. a list of lists would be converted to a simple list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actions** are operations that return an object to the driver program or write to external storage, they kick a computation. *Examples:* first, count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print \"Word count 2:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.persist()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 1:\",time.time() - t1\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.count()\n",
    "print \"Word count persisted 2:\", time.time() - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions are the operations that return a **final value** to the driver program or write data to an external storage system. \n",
    "\n",
    "Actions **force the evaluation** of the transformations required for the **RDD** they were called on, since they need to actually produce output.\n",
    "\n",
    "Returning to the previous example, until we call count over words and words persisted, the RDD are not computed. See that we persisted words_persisted, and until its second computation we cannot see the impact of persisting that RDD in memory.\n",
    "\n",
    "If we want to see a part of the RDD, we can use take, and to have the full RDD we can use collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "print \"Three elements\", lines.take(3)\n",
    "print \"The whole RDD\", lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing functions to Spark\n",
    "\n",
    "Most of Spark’s transformations, and some of its actions, depend on **passing in functions** that are used by Spark to **compute** data.\n",
    "\n",
    "In Python, we have three options for passing functions into Spark. \n",
    " * For shorter functions, we can pass in lambda expressions\n",
    " * We can pass in top-level functions, or \n",
    " * Locally defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'', u'individuum 1', u'individuum 2', u'individuum 3', u'individuum 4', u'individuum 5', u'individuum 6', u'individuum 7', u'individuum 8', u'individuum 9', u'individuum 10']\n",
      "[u'', u'individuum 1', u'individuum 2', u'individuum 3', u'individuum 4', u'individuum 5', u'individuum 6', u'individuum 7', u'individuum 8', u'individuum 9', u'individuum 10']\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "\n",
    "first_cells = lines.map(lambda x: x.split(\",\")[0])\n",
    "print first_cells.collect()\n",
    "\n",
    "# how to pass extra arguments\n",
    "def get_cell(x):\n",
    "    return x.split(\",\")[0]\n",
    "first_cells = lines.map(get_cell)\n",
    "print first_cells.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with common Spark transformations\n",
    "\n",
    "The two most common transformations you will likely be using are map() and filter(). \n",
    "\n",
    "The **map()** transformation takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD. \n",
    "\n",
    "The **filter()** transformation takes in a function and returns an RDD that only has elements that pass the filter() function.\n",
    "\n",
    "Sometimes ** map() ** returns nested lists, to flatten these nested lists we can use ** flatMap() **. So, ** flatMap() ** is called individually for each element in our input RDD. Instead of returning a single element, we return an iterator with our return values. Rather than producing an RDD of iterators, we get back an RDD that consists of the elements from all of the iterators.\n",
    "\n",
    "### Set operations\n",
    "\n",
    "* **distinct()** transformation to produce a new RDD with only distinct items. Note that distinct() is expensive, however, as it requires shuffling all the data over the network to ensure that we receive only one copy of each element. \n",
    "\n",
    "* **RDD.union(other)** back an RDD consisting of the data from both sources. Unlike the mathematical union(), if there are duplicates in the input RDDs, the result of Spark’s union() will contain duplicates (which we can fix if desired with distinct()).\n",
    "\n",
    "* **RDD.intersection(other)**  returns only elements in both RDDs. intersection() also removes all duplicates (including duplicates from a single RDD) while running. While intersection() and union() are two similar concepts, the performance of intersection() is much worse since it requires a shuffle over the network to identify common elements.\n",
    "\n",
    "* ** RDD.subtract(other)** function takes in another RDD and returns an RDD that has only values present in the first RDD and not the second RDD. Like intersection(), it performs a shuffle.\n",
    "\n",
    "* ** RDD.cartesian(other) ** transformation returns all possible pairs of (a,b) where a is in the source RDD and b is in the other RDD. The Cartesian product can be useful when we wish to consider the similarity between all possible pairs, such as computing every user’s expected interest in each offer. We can also take the Cartesian product of an RDD with itself, which can be useful for tasks like user similarity. Be warned, however, that the Cartesian product is very expensive for large Dds.\n",
    "\n",
    "### Actions\n",
    "\n",
    "* **reduce():** which takes a function that operates on two elements of the type in your RFD and returns a new element of the same type. \n",
    "\n",
    "* **aggregate():** takes an initial zero value of the type we want to return. We then supply a function to combine the elements from our RDD with the accumulator. Finally, we need to supply a second function to merge two accumulators, given that each node accumulates its own results locally. To know more:\n",
    "    * http://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark\n",
    "    * http://atlantageek.com/2015/05/30/python-aggregate-rdd/\n",
    "    \n",
    "* **collect():** returns the entire RDD’s contents. collect() is commonly used in unit tests where the entire contents of the RDD are expected to fit in memory, as that makes it easy to compare the value of our RDD with our expected result.\n",
    "\n",
    "* **take(n):** returns n elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection\n",
    "\n",
    "* **top():** will use the default ordering on the data, but we can supply our own comparison function to extract the top elements. \n",
    "\n",
    "\n",
    "### Exercises\n",
    "\n",
    "** Exercise 1: ** Download all books, from books.csv using the map function.\n",
    "\n",
    "** Exercise 2: ** Identify transformations and actions. When the returned data is calculated?\n",
    "\n",
    "** Exercise 3: ** Imagine that you only want to download Dickens books, how would you do that? Which is the impact of not persisting dickens_books_content?\n",
    "\n",
    "** Exercise 4: ** Use flatMap() in the resulting RDD of the previous exercise, how the result is different?\n",
    "\n",
    "** Exercise 5: ** You want to know the different books authors there are.\n",
    "\n",
    "** Exercise 6: ** Return Poe's and Dickens' books URLs (use union function).\n",
    "\n",
    "** Exercise 7: ** Return the list of books without Dickens' and Poe's books.\n",
    "\n",
    "** Exercise 8: ** Count the number of books using reduce function.\n",
    "\n",
    "** Exercise 9: ** Compute the mean price of estates from csv containing Sacramento's estate price using aggregate function.\n",
    "\n",
    "** Exercise 10: ** Get top 5 highest and lowest prices in Sacramento estate's transactions\n",
    "\n",
    "** Answer 1: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'http://www.textfiles.com/etext/REFERENCE/15-songs.txt', u'15-songs.txt', u'17619', u'A Civil War Songbook (January 1990)'], [u'http://www.textfiles.com/etext/REFERENCE/1776-va.rts', u'1776-va.rts', u'5907', u'The Virginia Declaration of Rights'], [u'http://www.textfiles.com/etext/REFERENCE/1mlkd11.txt', u'1mlkd11.txt', u'817486', u'\"Project Gutenberg: Martin Luther King\\'s \"\"I have a Dream\"\" Speech\"'], [u'http://www.textfiles.com/etext/REFERENCE/1st_than.txt', u'1st_than.txt', u'2979', u'\"The First Thanksgiving Proclomation', u' June 20', u' 1676\"'], [u'http://www.textfiles.com/etext/REFERENCE/2sqrt10a.txt', u'2sqrt10a.txt', u'5262079', u'\"Project Gutenberg: The Square Root of Two', u' to 5 Million digits\"'], [u'http://www.textfiles.com/etext/REFERENCE/32pri10.txt', u'32pri10.txt', u'247391', u'Project Gutenberg: The 32nd Mersenne prime'], [u'http://www.textfiles.com/etext/REFERENCE/all11.txt', u'all11.txt', u'85580', u'Project Gutenberg: The Declaration of Independence of The United States of America'], [u'http://www.textfiles.com/etext/REFERENCE/berne10.txt', u'berne10.txt', u'63397', u'Project Gutenberg: The Berne Copyright Convention'], [u'http://www.textfiles.com/etext/REFERENCE/bill11.txt', u'bill11.txt', u'2974', u'The United States Bill of Rights'], [u'http://www.textfiles.com/etext/REFERENCE/charlrsl.txt', u'charlrsl.txt', u'8446', u'\"The Charlotte Town Resolves: Resolves Adopted in Charlotte Town', u' North Carolina', u' May 31', u' 1775\"']]\n",
      "Newsgroups: freenet.shrine.songs\n",
      "From: aa300 (Jerry Murphy)\n",
      "Subject: Songs from the Civil War\n",
      "Dat\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "\n",
    "def download_file(csv_line):\n",
    "    link = csv_line[0]\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', link, preload_content=False)\n",
    "    response = r.read()\n",
    "    return response\n",
    "    \n",
    "books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "print books_info.take(10)\n",
    "\n",
    "books_content = books_info.map(download_file)\n",
    "print books_content.take(1)[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 2: **\n",
    "If we consider the text reading as a transformation...\n",
    "Transformations:\n",
    "* books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "* books_content = books_info.map(lambda x: download_file(x[0]))\n",
    "\n",
    "Actions:\n",
    "* print books_info.take(10)\n",
    "* print books_content.take(1)[0][:100]\n",
    "\n",
    "Computation is carried out in actions. In this case we take advantage of it, as for downloading data we only apply the function to one element of the books_content RDD\n",
    "\n",
    "** Answer 3: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'http://www.textfiles.com/etext/AUTHORS/DICKENS/dickens-american-631.txt', u'dickens-american-631.txt', u'604047', u'\"PROJECT GUTENBERG: American Notes for General Circulation', u' by Charles Dickens\"'], [u'http://www.textfiles.com/etext/AUTHORS/DICKENS/dickens-battle-630.txt', u'dickens-battle-630.txt', u'181551', u'\"PROJECT GUTENBERG: The Battle of Life', u' by Charles Dickens\"'], [u'http://www.textfiles.com/etext/AUTHORS/DICKENS/dickens-childs-629.txt', u'dickens-childs-629.txt', u'934709', u'\"PROJECT GUTENBERG: A Child\\'s History of England', u' by Charles Dickens\"'], [u'http://www.textfiles.com/etext/AUTHORS/DICKENS/dickens-chimes-379.txt', u'dickens-chimes-379.txt', u'170704', u'\"The Chimes', u' by Charles Dickens\"']]\n",
      "Project Gutenberg Etext of The Battle of Life by Charles Dickens\n",
      "#10 in our series by Charles Dicken\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dickens(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/DICKENS/\",link)\n",
    "    return t != None\n",
    "\n",
    "dickens_books_info = books_info.filter(is_dickens)\n",
    "print dickens_books_info.take(4)\n",
    "\n",
    "dickens_books_content = dickens_books_info.map(download_file)\n",
    "\n",
    "# take into consideration that each time an action is performed over dickens_book_content, the file is downloaded\n",
    "# this has a big impact into calculations\n",
    "print dickens_books_content.take(2)[1][:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 4: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'http://www.textfiles.com/etext/AUTHORS/DICKENS/dickens-american-631.txt', u'dickens-american-631.txt', u'604047', u'\"PROJECT GUTENBERG: American Notes for General Circulation']\n"
     ]
    }
   ],
   "source": [
    "flat_content = dickens_books_info.flatMap(lambda x: x)\n",
    "print flat_content.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 5: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'BURROUGHS',\n",
       " u'DICKENS',\n",
       " u'STEVENSON',\n",
       " u'TWAIN',\n",
       " u'EMERSON',\n",
       " u'WILDE',\n",
       " u'ARISTOTLE',\n",
       " u'DOYLE',\n",
       " u'KANT',\n",
       " u'UNKNOWN',\n",
       " u'HAWTHORNE',\n",
       " u'PLATO',\n",
       " u'IRVING',\n",
       " u'KEATS',\n",
       " u'JEFFERSON',\n",
       " u'SHAKESPEARE',\n",
       " u'POE',\n",
       " u'MILTON']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_author(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return t.group(1)\n",
    "    return u'UNKNOWN'\n",
    "\n",
    "authors = books_info.map(get_author)\n",
    "authors.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Answer 6 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/blackcat.poe'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/masque_r.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-angel-666.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-dreamland-434.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-literary-454.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-morning-559.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-morning-559.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-to-715.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/poe-to-718.txt'),\n",
       " (u'POE', u'http://www.textfiles.com/etext/AUTHORS/POE/telltale.txt'),\n",
       " (u'DICKENS',\n",
       "  u'http://www.textfiles.com/etext/AUTHORS/DICKENS/dickens-pickwick-635.txt')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_author_and_link(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return (t.group(1), link)\n",
    "    return (u'UNKNOWN',link)\n",
    "\n",
    "authors_links = books_info.map(get_author_and_link)\n",
    "\n",
    "# not very efficient\n",
    "dickens_books = authors_links.filter(lambda x: x[0]==\"DICKENS\")\n",
    "poes_books = authors_links.filter(lambda x: x[0]==\"POE\")\n",
    "\n",
    "poes_dickens_books = poes_books.union(dickens_books)\n",
    "poes_dickens_books.sample(True,0.05).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 7 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'EMERSON',\n",
       " u'WILDE',\n",
       " u'JEFFERSON',\n",
       " u'MILTON',\n",
       " u'STEVENSON',\n",
       " u'DOYLE',\n",
       " u'ARISTOTLE',\n",
       " u'UNKNOWN',\n",
       " u'PLATO',\n",
       " u'IRVING',\n",
       " u'KEATS',\n",
       " u'HAWTHORNE',\n",
       " u'SHAKESPEARE',\n",
       " u'BURROUGHS',\n",
       " u'TWAIN',\n",
       " u'KANT']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_links.subtract(poes_dickens_books).map(lambda x: x[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 8 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_links.map(lambda x: 1).reduce(lambda x,y: x+y) == authors_links.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234144.26395939087"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sacramento_estate_csv = sc.textFile(\"../data/Sacramentorealestatetransactions.csv\")\n",
    "header = sacramento_estate_csv.first()\n",
    "\n",
    "sacramento_estate = sacramento_estate_csv.filter(lambda x: x != header)\\\n",
    "        .map(lambda x: x.split(\",\"))\\\n",
    "        .map(lambda x: int(x[9]))\n",
    "\n",
    "seqOp = (lambda x,y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "total_sum, number = sacramento_estate.aggregate((0,0),seqOp,combOp)\n",
    "mean = float(total_sum)/number\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[884790, 879000, 839000, 830000, 760000]\n",
      "[1551, 2000, 4897, 4897, 4897]\n"
     ]
    }
   ],
   "source": [
    "print sacramento_estate.top(5)\n",
    "print sacramento_estate.top(5, key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spark Key/Value Pairs\n",
    "\n",
    "Spark provides special operations on RDDs containing key/value pairs. \n",
    "\n",
    "These RDDs are called pair RDDs, but are simple RDDs with an special structure. In Python, for the functions on keyed data to work we need to return an RDD composed of tuples.\n",
    "\n",
    "** Exercise 1:** Create a pair RDD from our books information data, having author as key and the rest of the information as value. (Hint: the answer is very similar to the previous section Exercise 6)\n",
    "\n",
    "** Exercise 2:** Check that pair RDDs are also RDDs and that common RDD operations work as well. Filter elements with author equals to \"UNKNOWN\" from previous RDD. \n",
    "\n",
    "** Exercise 3:** Check mapValue in Spark API (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapValues) function that works on pair RDDs.\n",
    "\n",
    "** Answer 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'UNKNOWN', [u'http://www.textfiles.com/etext/REFERENCE/15-songs.txt', u'15-songs.txt', u'17619', u'A Civil War Songbook (January 1990)']), (u'UNKNOWN', [u'http://www.textfiles.com/etext/REFERENCE/1776-va.rts', u'1776-va.rts', u'5907', u'The Virginia Declaration of Rights']), (u'UNKNOWN', [u'http://www.textfiles.com/etext/REFERENCE/1mlkd11.txt', u'1mlkd11.txt', u'817486', u'\"Project Gutenberg: Martin Luther King\\'s \"\"I have a Dream\"\" Speech\"']), (u'UNKNOWN', [u'http://www.textfiles.com/etext/REFERENCE/1st_than.txt', u'1st_than.txt', u'2979', u'\"The First Thanksgiving Proclomation', u' June 20', u' 1676\"']), (u'UNKNOWN', [u'http://www.textfiles.com/etext/REFERENCE/2sqrt10a.txt', u'2sqrt10a.txt', u'5262079', u'\"Project Gutenberg: The Square Root of Two', u' to 5 Million digits\"'])]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_author_data(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return (t.group(1), csv_line)\n",
    "    return (u'UNKNOWN', csv_line)\n",
    "\n",
    "books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "authors_info = books_info.map(get_author_data)\n",
    "\n",
    "print authors_info.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Answer 2: **\n",
    "\n",
    "The operations over pair RDDs will also be slightly different.\n",
    "\n",
    "But take into account that pair RDDs are just *special* RDDs that some operations can be applied, however common RDDs also fork for them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WILDE',\n",
       "  [u'http://www.textfiles.com/etext/AUTHORS/WILDE/wilde-ballad-611.txt',\n",
       "   u'wilde-ballad-611.txt',\n",
       "   u'27238',\n",
       "   u'\"The Ballad of Reading Gaol',\n",
       "   u' by Oscar Wilde (1898)\"']),\n",
       " (u'WILDE',\n",
       "  [u'http://www.textfiles.com/etext/AUTHORS/WILDE/wilde-burden-612.txt',\n",
       "   u'wilde-burden-612.txt',\n",
       "   u'17887',\n",
       "   u'\"The Burden of Itys',\n",
       "   u' by Oscar Wilde (1890)\"']),\n",
       " (u'WILDE',\n",
       "  [u'http://www.textfiles.com/etext/AUTHORS/WILDE/wilde-charmides-601.txt',\n",
       "   u'wilde-charmides-601.txt',\n",
       "   u'34648',\n",
       "   u'\"Charmides',\n",
       "   u' by Oscar Wilde (1890)\"'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_info.filter(lambda x: x[0] != \"UNKNOWN\").take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 3:** \n",
    "\n",
    "Sometimes is awkward to work with pairs, and Spark provides a map function that operates over values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'UNKNOWN', u'17619'),\n",
       " (u'UNKNOWN', u'5907'),\n",
       " (u'UNKNOWN', u'817486'),\n",
       " (u'UNKNOWN', u'2979'),\n",
       " (u'UNKNOWN', u'5262079')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_info.mapValues(lambda x: x[2]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on Pair RDDs\n",
    "\n",
    " Since pair RDDs contain tuples, we need to pass functions that operate on tuples rather than on individual elements.\n",
    " \n",
    " * *reduceByKey(func):* Combine values with the same key.\n",
    " * *groupByKey():* Group values with the same key.\n",
    " * *combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner):* Combine values with the same key using a different result type.\n",
    " * *keys():* return RDD keys\n",
    " * *values():* return RDD values\n",
    " * *groupBy():* takes a function that it applies to every element in the source RDD and uses the result to determine the key.\n",
    " * *cogroup():* over two RDDs sharing the same key type, K, with the respective value types V and W gives us back RDD[(K,(Iterable[V], Iterable[W]))]. If one of the RDDs doesn’t have elements for a given key that is present in the other RDD, the corresponding Iterable is simply empty. cogroup() gives us the power to group data from multiple RDDs.\n",
    "\n",
    "** Exercise 1: ** Get the total size of files for each author. \n",
    "\n",
    "** Exercise 2: ** Get the top 5 authors with more data.\n",
    "\n",
    "** Exercise 3:** Try the combineByKey() with a randomly generated set of 5 values for 4 keys. Get the average value of the random variable for each key.\n",
    "\n",
    "** Exercise 4:** Compute the average book size per author using combineByKey(). If you were an English Literature student and your teacher says: \"Pick one Author and I'll randomly pick a book for you to read\", what would be a Data Scientist answer?\n",
    "\n",
    "** Exercise 5: ** All Spark books have the word count example. Let's count words over all our books! (This might take some time)\n",
    "\n",
    "** Exercise 6: ** Group author data by author surname initial. How many authors have we grouped? \n",
    "\n",
    "** Exercise 7: ** Generate a pair RDD with alphabet letters in upper case as key, and empty list as value. Then group the previous RDD with this new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'BURROUGHS', 10070497),\n",
       " (u'DICKENS', 14236826),\n",
       " (u'STEVENSON', 6965452),\n",
       " (u'TWAIN', 13259786),\n",
       " (u'EMERSON', 2655619),\n",
       " (u'WILDE', 669926),\n",
       " (u'ARISTOTLE', 6219825),\n",
       " (u'DOYLE', 8450256),\n",
       " (u'KANT', 5901915),\n",
       " (u'UNKNOWN', 266421132),\n",
       " (u'HAWTHORNE', 1898878),\n",
       " (u'PLATO', 3648947),\n",
       " (u'IRVING', 2223565),\n",
       " (u'KEATS', 360556),\n",
       " (u'JEFFERSON', 6565921),\n",
       " (u'SHAKESPEARE', 5347823),\n",
       " (u'POE', 3395985),\n",
       " (u'MILTON', 911458)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_data = authors_info.mapValues(lambda x: int(x[2]))\n",
    "authors_data.reduceByKey(lambda y,x: y+x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'UNKNOWN', 266421132),\n",
       " (u'DICKENS', 14236826),\n",
       " (u'TWAIN', 13259786),\n",
       " (u'BURROUGHS', 10070497),\n",
       " (u'DOYLE', 8450256)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aauthors_data.reduceByKey(lambda y,x: y+x).top(5,key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.36496102496138977),\n",
       " (4, 0.11892759636084722),\n",
       " (1, -0.66545959341697147),\n",
       " (2, 0.30208685716557426),\n",
       " (3, -0.30334391176249054)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate the data\n",
    "rdd = sc.parallelize(zip(range(5)*5, np.random.normal(0,1,5*5)))\n",
    "\n",
    "createCombiner = lambda value: (value,1)\n",
    "# you can check what createCombiner does\n",
    "# rdd.mapValues(createCombiner).collect()\n",
    "\n",
    "# here x is the combiner (sum,count) and value is value in the \n",
    "# initial RDD (the random variable)\n",
    "mergeValue = lambda x, value: (x[0] + value, x[1] + 1)\n",
    "\n",
    "# here, all combiners are summed (sum,count)\n",
    "mergeCombiner = lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "sumCount = rdd.combineByKey(createCombiner,\n",
    "                        mergeValue,\n",
    "                         mergeCombiner)\n",
    "\n",
    "sumCount.mapValues(lambda x: x[0]/x[1]).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'KEATS', 10604),\n",
       " (u'POE', 24431),\n",
       " (u'MILTON', 30381),\n",
       " (u'WILDE', 39407),\n",
       " (u'IRVING', 55589)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createCombiner = lambda value: (value,1)\n",
    "# you can check what createCombiner does\n",
    "# rdd.mapValues(createCombiner).collect()\n",
    "\n",
    "# here x is the combiner (sum,count) and value is value in the \n",
    "# initial RDD (the random variable)\n",
    "mergeValue = lambda x, value: (x[0] + value, x[1] + 1)\n",
    "\n",
    "# here, all combiners are summed (sum,count)\n",
    "mergeCombiner = lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "sumCount = authors_data.combineByKey(createCombiner,\n",
    "                        mergeValue,\n",
    "                         mergeCombiner)\n",
    "\n",
    "sumCount.mapValues(lambda x: x[0]/x[1]).collect()\n",
    "# I would choose the author with lowest average book size\n",
    "sumCount.mapValues(lambda x: x[0]/x[1]).top(5,lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "** Answer 5: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3278274),\n",
       " ('of', 1881320),\n",
       " ('and', 1853270),\n",
       " ('to', 1467214),\n",
       " ('a', 1139576)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib3\n",
    "import re\n",
    "\n",
    "def download_file(csv_line):\n",
    "    link = csv_line[0]\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', link, preload_content=False)\n",
    "    response = r.read()\n",
    "    return response\n",
    "    \n",
    "books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "books_content = books_info.map(download_file)\n",
    "#books_content = sc.parallelize(books_info.map(download_file).take(2))\n",
    "\n",
    "words_rdd = books_content.flatMap(lambda x: x.split(\" \")).\\\n",
    "                          flatMap(lambda x: x.split(\"\\r\\n\")).\\\n",
    "                          map(lambda x: re.sub('[^0-9a-zA-Z]+', '', x).lower()).\\\n",
    "                          filter(lambda x: x != '')\n",
    "\n",
    "words_rdd.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).top(5, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 6: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'A', <pyspark.resultiterable.ResultIterable object at 0x0000000006E90B70>), (u'E', <pyspark.resultiterable.ResultIterable object at 0x0000000006E90EB8>), (u'I', <pyspark.resultiterable.ResultIterable object at 0x0000000006E90630>), (u'K', <pyspark.resultiterable.ResultIterable object at 0x0000000006E90080>), (u'M', <pyspark.resultiterable.ResultIterable object at 0x0000000006E2F080>), (u'S', <pyspark.resultiterable.ResultIterable object at 0x0000000006E2F198>), (u'U', <pyspark.resultiterable.ResultIterable object at 0x0000000006E50198>), (u'W', <pyspark.resultiterable.ResultIterable object at 0x0000000006E507F0>), (u'B', <pyspark.resultiterable.ResultIterable object at 0x0000000006E50940>), (u'D', <pyspark.resultiterable.ResultIterable object at 0x0000000006E50E80>), (u'H', <pyspark.resultiterable.ResultIterable object at 0x000000000664E2E8>), (u'J', <pyspark.resultiterable.ResultIterable object at 0x000000000664E630>), (u'P', <pyspark.resultiterable.ResultIterable object at 0x000000000664E7F0>), (u'T', <pyspark.resultiterable.ResultIterable object at 0x0000000006FAD7F0>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'K', 2), (u'S', 2), (u'D', 2), (u'P', 2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print authors_info.groupBy(lambda x: x[0][0]).collect()\n",
    "\n",
    "authors_info.map(lambda x: x[0]).distinct().\\\n",
    "                map(lambda x: (x[0],1)).\\\n",
    "                reduceByKey(lambda x,y: x+y).\\\n",
    "                filter(lambda x: x[1]>1).\\\n",
    "                collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer 7:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x6dcad68>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x6dcaf28>)),\n",
       " ('C',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x6dd62e8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x6dd6320>)),\n",
       " ('E',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x6dd6358>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x6dd6390>)),\n",
       " ('G',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x6dd6780>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x6dd67b8>)),\n",
       " ('I',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x6dd67f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x6dd6828>))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "sc.parallelize(list(string.ascii_uppercase)).\\\n",
    "                                map(lambda x: (x,[])).\\\n",
    "                                cogroup(authors_info.groupBy(lambda x: x[0][0])).\\\n",
    "                                take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Some of the most useful operations we get with keyed data comes from using it together with other keyed data. \n",
    "\n",
    "Joining data together is probably one of the most common operations on a pair RDD, and we have a full range of options including right and left outer joins, cross joins, and inner joins.\n",
    "\n",
    "### Inner Join\n",
    "\n",
    "Only keys that are present in both pair RDDs are output. \n",
    "\n",
    "When there are multiple values for the same key in one of the inputs, the resulting pair RDD will have an entry for every possible pair of values with that key from the two input RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Take countries_data_clean.csv and countries_GDP_clean.csv and join them using country name as key. Before doing the join, please, check how many element should the resulting pair RDD have. After the join, check if the initial hypothesis was true. In case it is not, what is the reason? How would you resolve that problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final number of elements in the joined rdd should be:  229\n",
      "[(u'Afghanistan', [u'0', u'Afghanistan', u'Afghanistan', u'Afganistan/Afqanestan', u'AF', u'Asia', u'', u'32564342', u'652230', u'0', u'islamic republic', u'Afghani', u'AFN', u'93', u'38.6', u'13.9', u'50.9', u'https://www.laenderdaten.info/Asien/Afghanistan/index.php'])]\n",
      "[(u'United States', [u'0', u'USA', u'1', u'United States', u'17946996.0'])]\n",
      "The initial hypothesis is  False\n",
      "The final joined rdd size is  185\n"
     ]
    }
   ],
   "source": [
    "#more info: https://www.worlddata.info/downloads/\n",
    "rdd_countries = sc.textFile(\"../data/countries_data_clean.csv\").map(lambda x: x.split(\",\"))\n",
    "#more info: http://data.worldbank.org/data-catalog/GDP-ranking-table\n",
    "rdd_gdp = sc.textFile(\"../data/countries_GDP_clean.csv\").map(lambda x: x.split(\",\"))\n",
    "\n",
    "# check rdds size\n",
    "hyp_final_rdd_num = rdd_gdp.count() if rdd_countries.count() > rdd_gdp.count() else rdd_countries.count()\n",
    "print \"The final number of elements in the joined rdd should be: \", hyp_final_rdd_num\n",
    "p_rdd_gdp = rdd_gdp.map(lambda x: (x[3],x))\n",
    "p_rdd_countries = rdd_countries.map(lambda x: (x[1],x))\n",
    "\n",
    "print p_rdd_countries.take(1)\n",
    "print p_rdd_gdp.take(1)\n",
    "\n",
    "p_rdd_contry_data = p_rdd_countries.join(p_rdd_gdp)\n",
    "\n",
    "final_join_rdd_size = p_rdd_contry_data.count()\n",
    "hyp = hyp_final_rdd_num == final_join_rdd_size\n",
    "print \"The initial hypothesis is \", hyp\n",
    "if not hyp:\n",
    "    print \"The final joined rdd size is \", final_join_rdd_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left and Right outer Joins\n",
    "\n",
    "Sometimes we don’t need the key to be present in both RDDs to want it in our result.\n",
    "\n",
    "For example, imagine that our list of countries is not complete, and we don't want to miss data if it a country is not present in both RDDs.\n",
    "\n",
    "** leftOuterJoin(other)** and **rightOuterJoin(other)** both join pair RDDs together by key, where one of the pair RDDs can be missing the key.\n",
    "\n",
    "With **leftOuterJoin()** the resulting pair RDD has entries for each key in the source RDD. The value associated with each key in the result is a tuple of the value from the source RDD and an Option for the value from the other pair RDD. In Python, if a value isn’t present None is used; and if the value is present the regular value, without any wrapper, is used. As with join(), we can have multiple entries for each key; when this occurs, we get the Cartesian product between the two lists of values.\n",
    "\n",
    "**rightOuterJoin()** is almost identical to leftOuterJoin() except the key must be present in the other RDD and the tuple has an option for the source rather than the other RDD.\n",
    "\n",
    "** Exercise: **\n",
    "\n",
    "Use two simple RDDs to show the results of left and right outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_1:  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "rdd_2:  [(0, 1), (2, 1), (4, 1), (6, 1), (8, 1)]\n",
      "leftOuterJoin:  [(0, (1, 1)), (1, (1, None)), (2, (1, 1)), (3, (1, None)), (4, (1, 1))]\n",
      "rightOuterJoin:  [(0, (1, 1)), (8, (None, 1)), (2, (1, 1)), (4, (1, 1)), (6, (None, 1))]\n",
      "join:  [(0, (1, 1)), (2, (1, 1)), (4, (1, 1))]\n",
      "rdd_3:  [(0, 1), (2, 1), (4, 1), (6, 1), (8, 1), (4, 2), (6, 4)]\n",
      "join:  [(0, (1, 1)), (8, (1, 1)), (2, (1, 1)), (4, (1, 1)), (4, (1, 2)), (6, (1, 1)), (6, (1, 4))]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "rdd_1 = sc.parallelize([(x,1) for x in range(n)])\n",
    "rdd_2 = sc.parallelize([(x*2,1) for x in range(n)])\n",
    "\n",
    "print \"rdd_1: \",rdd_1.collect()\n",
    "print \"rdd_2: \",rdd_2.collect()\n",
    "\n",
    "print \"leftOuterJoin: \",rdd_1.leftOuterJoin(rdd_2).collect()\n",
    "print \"rightOuterJoin: \",rdd_1.rightOuterJoin(rdd_2).collect()\n",
    "\n",
    "print \"join: \", rdd_1.join(rdd_2).collect()\n",
    "\n",
    "#explore what hapens if a key is present twice or more\n",
    "rdd_3 = sc.parallelize([(x*2,1) for x in range(n)] + [(4,2),(6,4)])\n",
    "print \"rdd_3: \",rdd_3.collect()\n",
    "print \"join: \", rdd_2.join(rdd_3).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise: **\n",
    "\n",
    "Generate two pair RDDs with country info:\n",
    "1. A first one with country code and GDP\n",
    "2. A second one with country code and life expectancy\n",
    "\n",
    "Then join them to have a pair RDD with country code plus GDP and life expentancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer: **\n",
    "\n",
    "Inspect the dataset with GDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'330779.0']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_gdp = sc.textFile(\"../data/countries_GDP_clean.csv\").map(lambda x: x.split(\";\"))\n",
    "rdd_gdp.take(2)\n",
    "#generate a pair rdd with countrycode and GDP\n",
    "rdd_cc_gdp = rdd_gdp.map(lambda x: (x[1],x[4]))\n",
    "rdd_cc_gdp.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the dataset with life expectancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'Afghanistan', u'Afghanistan', u'Afganistan/Afqanestan', u'AF', u'Asia', u'', u'32564342', u'652230', u'0', u'islamic republic', u'Afghani', u'AFN', u'93', u'38.6', u'13.9', u'50.9', u'https://www.laenderdaten.info/Asien/Afghanistan/index.php'], [u'1', u'Egypt', u'\\xc4gypten', u'Misr', u'EG', u'Africa', u'', u'88487396', u'1001450', u'2450', u'republic', u'Pfund', u'EGP', u'20', u'22.9', u'4.8', u'73.7', u'https://www.laenderdaten.info/Afrika/Aegypten/index.php']]\n",
      "[(u'Afghanistan', u'AFG'), (u'Egypt', u'EGY')]\n",
      "247\n",
      "247\n",
      "247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'\"Saint Helena', (u'7.4', None)), (u'\"Micronesia', (u'4.2', None))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_countries = sc.textFile(\"../data/countries_data_clean.csv\").map(lambda x: x.split(\",\"))\n",
    "print rdd_countries.take(2)\n",
    "#generate a pair rdd with countrycode and lifexpectancy \n",
    "#(more info in https://www.worlddata.info/downloads/)\n",
    "#we don't have countrycode in this dataset, but let's try to add it\n",
    "#we have a dataset with countrynames and countrycodes\n",
    "#let's take countryname and ISO 3166-1 alpha3 code\n",
    "rdd_cc = sc.textFile(\"../data/countrycodes.csv\").\\\n",
    "                    map(lambda x: x.split(\";\")).\\\n",
    "                    map(lambda x: (x[0].strip(\"\\\"\"),x[4].strip(\"\\\"\"))).\\\n",
    "                    filter(lambda x: x[0] != 'Country (en)')\n",
    "print rdd_cc.take(2)\n",
    "rdd_cc_info = rdd_countries.map(lambda x: (x[1],x[16]))\n",
    "rdd_cc_info.take(2)\n",
    "#let's count and see if something is missing\n",
    "print rdd_cc.count()\n",
    "print rdd_cc_info.count()\n",
    "#take only values, the name is no longer needed\n",
    "rdd_name_cc_le = rdd_cc_info.leftOuterJoin(rdd_cc)\n",
    "rdd_cc_le = rdd_name_cc_le.map(lambda x: x[1])\n",
    "print rdd_cc_le.count()\n",
    "#what is missing?\n",
    "rdd_name_cc_le.filter(lambda x: x[1][1] == None).collect()\n",
    "#how can we solve this problem??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some missing data, that we have to complete, but we have quite a lot of data, let's follow.\n",
    "\n",
    "Inspect the results of GDP and life expectancy and join them. ** Is there some data missing? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there some data missing? True\n",
      "GDP dataset:  229\n",
      "Life expectancy dataset:  247\n",
      "[(u'81.8', u'CAN'), (u'73.5', u'BRA'), (u'82.1', u'ITA'), (u'80.2', u'FRO'), (u'68.1', u'IND'), (u'74.7', u'LTU'), (u'78.5', u'CYP'), (u'77.6', u'SXM'), (u'82.7', u'AND'), (u'75.1', u'VCT')]\n",
      "[(u'USA', u'17946996.0'), (u'CHN', u'10866444.0'), (u'JPN', u'4123258.0'), (u'DEU', u'3355772.0'), (u'GBR', u'2848755.0'), (u'FRA', u'2421682.0'), (u'IND', u'2073543.0'), (u'ITA', u'1814763.0'), (u'BRA', u'1774725.0'), (u'CAN', u'1550537.0')]\n"
     ]
    }
   ],
   "source": [
    "print \"Is there some data missing?\", rdd_cc_gdp.count() != rdd_cc_le.count()\n",
    "print \"GDP dataset: \", rdd_cc_gdp.count()\n",
    "print \"Life expectancy dataset: \", rdd_cc_le.count()\n",
    "#lets try to see what happens\n",
    "print rdd_cc_le.take(10)\n",
    "print  rdd_cc_gdp.take(10)\n",
    "rdd_cc_gdp_le = rdd_cc_le.map(lambda x: (x[1],x[0])).leftOuterJoin(rdd_cc_gdp)\n",
    "#we have some countries that the data is missing\n",
    "# we have to check if this data is available\n",
    "# or there is any error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Data\n",
    "\n",
    "sortByKey(): We can sort an RDD with key/value pairs provided that there is an ordering defined on the key.\n",
    "\n",
    "Once we have sorted our data, any subsequent call on the sorted data to collect() or save() will result in ordered data.\n",
    "\n",
    "** Exercise: **\n",
    "Sort country data by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'\"Micronesia',\n",
       "  ([u'138',\n",
       "    u'\"Micronesia',\n",
       "    u' Federated States of\"',\n",
       "    u'Mikronesien',\n",
       "    u'Micronesia',\n",
       "    u'FM',\n",
       "    u'Oceania',\n",
       "    u'',\n",
       "    u'105216',\n",
       "    u'702',\n",
       "    u'6112',\n",
       "    u'federal republic',\n",
       "    u'Dollar',\n",
       "    u'USD',\n",
       "    u'691',\n",
       "    u'20.5',\n",
       "    u'4.2',\n",
       "    u'72.6',\n",
       "    u'https://www.laenderdaten.info/Ozeanien/Mikronesien/index.php'],\n",
       "   [u'190', u'FSM', u'191', u'\"Micronesia', u' Fed. Sts.\"', u'318.0'])),\n",
       " (u'Afghanistan',\n",
       "  ([u'0',\n",
       "    u'Afghanistan',\n",
       "    u'Afghanistan',\n",
       "    u'Afganistan/Afqanestan',\n",
       "    u'AF',\n",
       "    u'Asia',\n",
       "    u'',\n",
       "    u'32564342',\n",
       "    u'652230',\n",
       "    u'0',\n",
       "    u'islamic republic',\n",
       "    u'Afghani',\n",
       "    u'AFN',\n",
       "    u'93',\n",
       "    u'38.6',\n",
       "    u'13.9',\n",
       "    u'50.9',\n",
       "    u'https://www.laenderdaten.info/Asien/Afghanistan/index.php'],\n",
       "   [u'109', u'AFG', u'110', u'Afghanistan', u'19199.0']))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_rdd_contry_data.sortByKey().take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions over Pair RDDs\n",
    "\n",
    "* countByKey(): Count the number of elements for each key.\n",
    "* collectAsMap(): Collect the result as a map to provide easy lookup.\n",
    "* lookup(key): Return all values associated with the provided key.\n",
    "\n",
    "** Exercises: **\n",
    "    1. Count countries RDD by key\n",
    "    2. Collect countries RDD as map\n",
    "    3. Lookup Andorra info in countries RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_rdd_contry_data.countByKey()[\"Andorra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([u'5',\n",
       "  u'Andorra',\n",
       "  u'Andorra',\n",
       "  u'Andorra',\n",
       "  u'AD',\n",
       "  u'Europe',\n",
       "  u'',\n",
       "  u'85580',\n",
       "  u'468',\n",
       "  u'0',\n",
       "  u'constitutional monarchy',\n",
       "  u'Euro',\n",
       "  u'EUR',\n",
       "  u'376',\n",
       "  u'8.1',\n",
       "  u'7.0',\n",
       "  u'82.7',\n",
       "  u'https://www.laenderdaten.info/Europa/Andorra/index.php'],\n",
       " [u'161', u'ADO', u'162', u'Andorra', u'3249.0'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_rdd_contry_data.collectAsMap()[\"Andorra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([u'5',\n",
       "   u'Andorra',\n",
       "   u'Andorra',\n",
       "   u'Andorra',\n",
       "   u'AD',\n",
       "   u'Europe',\n",
       "   u'',\n",
       "   u'85580',\n",
       "   u'468',\n",
       "   u'0',\n",
       "   u'constitutional monarchy',\n",
       "   u'Euro',\n",
       "   u'EUR',\n",
       "   u'376',\n",
       "   u'8.1',\n",
       "   u'7.0',\n",
       "   u'82.7',\n",
       "   u'https://www.laenderdaten.info/Europa/Andorra/index.php'],\n",
       "  [u'161', u'ADO', u'162', u'Andorra', u'3249.0'])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_rdd_contry_data.lookup(\"Andorra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "\n",
    "(*from: Learning Spark - O'Reilly*)\n",
    "\n",
    "Spark programs can choose to control their RDDs’ partitioning to reduce communication. \n",
    "\n",
    "Partitioning will not be helpful in all applications— for example, if a given RDD is scanned only once, there is no point in partitioning it in advance. \n",
    "\n",
    "It is useful only when a dataset is reused multiple times in key-oriented operations such as joins.\n",
    "\n",
    "Spark’s partitioning is **available on all RDDs of key/value pairs**, and causes the system to **group elements based on a function of each key**.\n",
    "\n",
    "Spark does not give explicit control of which worker node each key goes to (partly because the system is designed to work even if specific nodes fail), it lets the program ensure that a set of keys will appear together on some node. \n",
    "\n",
    "** Example:**\n",
    "\n",
    "As a simple example, consider an application that keeps a large table of user information in memory—say, an RDD of (UserID, UserInfo) pairs, where UserInfo contains a list of topics the user is subscribed to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'user0',\n",
       "  [u'healthcare',\n",
       "   u'sportscasters',\n",
       "   u'elections',\n",
       "   u'running',\n",
       "   u'environment',\n",
       "   u'internet',\n",
       "   u'airlines',\n",
       "   u'automotive',\n",
       "   u'directors',\n",
       "   u'history',\n",
       "   u'grammys']),\n",
       " (u'user1',\n",
       "  [u'stock exchange', u'directors', u'basketball', u'running', u'automotive'])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_userinfo = sc.textFile(\"../data/users_events_example/user_info_1000users_20topics.csv\")\\\n",
    "                    .filter(lambda x: len(x)>0)\\\n",
    "                    .map(lambda x: (x.split(\",\")[0],x.split(\",\")[1].split(\"|\")))\n",
    "rdd_userinfo.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application periodically combines this table with a smaller file representing events that happened in the past five minutes—say, a table of (UserID, LinkInfo) pairs for users who have clicked a link on a website in those five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'user67', [u'movies']), (u'user77', [u'stock exchange'])]\n"
     ]
    }
   ],
   "source": [
    "rdd_userevents = sc.textFile(\"../data/users_events_example/userevents_*.log\")\\\n",
    "                                .filter(lambda x: len(x))\\\n",
    "                                .map(lambda x: (x.split(\",\")[1], [x.split(\",\")[2]]))\n",
    "print rdd_userevents.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we may wish to count how many users visited a link that was not to one of their subscribed topics. We can perform this combination with Spark’s join() operation, which can be used to group the User Info and LinkInfo pairs for each UserID by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "8492\n",
      "1508\n"
     ]
    }
   ],
   "source": [
    "rdd_joined = rdd_userinfo.join(rdd_userevents)\n",
    "print rdd_joined.count()\n",
    "print rdd_joined.filter(lambda x: (x[1][1][0] not in x[1][0])).count()\n",
    "print rdd_joined.filter(lambda x: (x[1][1][0] in x[1][0])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we want to count the number of visits to non-subscribed visits using a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of visits to non-subscribed topics:  852\n"
     ]
    }
   ],
   "source": [
    "rdd_userinfo = sc.textFile(\"../data/users_events_example/user_info_1000users_20topics.csv\")\\\n",
    "                    .filter(lambda x: len(x)>0)\\\n",
    "                    .map(lambda x: (x.split(\",\")[0],x.split(\",\")[1].split(\"|\"))).persist()\n",
    "        \n",
    "def process_new_logs(event_fite_path):\n",
    "    rdd_userevents = sc.textFile(event_fite_path)\\\n",
    "                                .filter(lambda x: len(x))\\\n",
    "                                .map(lambda x: (x.split(\",\")[1], [x.split(\",\")[2]]))\n",
    "    rdd_joined = rdd_userinfo.join(rdd_userevents)\n",
    "    print \"Number of visits to non-subscribed topics: \" ,\\\n",
    "            rdd_joined.filter(lambda x: (x[1][1][0] not in x[1][0])).count()\n",
    "        \n",
    "process_new_logs(\"../data/users_events_example/userevents_01012016000500.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will run fine as is, but it will be inefficient. This is because the join() operation, called each time process_new_logs() is invoked, does not know anything about how the keys are partitioned in the datasets. By default, this operation will hash all  the keys of both datasets, sending elements with the same key hash across the network to the same machine, and then join together the elements with the same key on that machine (see figure below). \n",
    "![user partitioning example rdds](https://github.com/f-guitart/data_mining/blob/master/notes/img/user_partitioning_example_rdds.png?raw=true \"user partitioning example rdds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we expect the rdd_userinfo table to be much larger than the small log of events seen every five minutes, this wastes a lot of work: the rdd_userinfo table is hashed and shuffled across the network on every call, even though it doesn’t change.\n",
    "\n",
    "Fixing this is simple: just use the **partitionBy()** transformation on rdd_userinfo to hash-partition it at the start of the program. We do this by passing a spark.HashPartitioner object to partitionBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[614] at mapPartitions at PythonRDD.scala:422"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_userinfo = sc.textFile(\"../data/users_events_example/user_info_1000users_20topics.csv\")\\\n",
    "                    .filter(lambda x: len(x)>0)\\\n",
    "                    .map(lambda x: (x.split(\",\")[0],x.split(\",\")[1].split(\"|\"))).partitionBy(10)    \n",
    "rdd_userinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process_new_logs() method can remain unchanged: the rdd_userevents RDD is local to process_new_logs(), and is used only once within this method, so there is no advantage in specifying a partitioner for events. Because we called partitionBy() when building userData, Spark will now know that it is hash-partitioned, and calls to join() on it will take advantage of this information. In particular, when we call user rdd_userinfo.join(rdd_userevents), Spark will shuffle only the events RDD, sending events with each particular UserID to the machine that contains the corresponding hash partition of rdd_userinfo. The result is that a lot less data is communicated over the network, and the program runs significantly faster.\n",
    "\n",
    "![user partitioning example rdds 2](https://github.com/f-guitart/data_mining/blob/master/notes/img/user_partitioning_example_rdds_partitionBy.png?raw=true \"user partitioning example rdds 2\")\n",
    "\n",
    "More on partitioning:\n",
    " * http://stackoverflow.com/questions/35973590/pyspark-partioning-data-using-partitionby\n",
    " * http://stackoverflow.com/questions/31424396/apache-spark-hashpartitioner-how-does-it-work\n",
    "\n",
    "Note that partitionBy() is a transformation, so it always returns a new RDD—it does not change the original RDD in place. RDDs can never be modified once created. Therefore it is important to persist and save as rdd_userinfo the result of partitionBy(), not the original textFile().\n",
    "\n",
    "Also, the 100 passed to partitionBy() represents the number of partitions, which will control how many parallel tasks perform further operations on the RDD (e.g., joins); in general, **make this at least as large as the number of cores in your cluster**.\n",
    "\n",
    "In fact, many other Spark operations automatically result in an RDD with known partitioning information, and many operations other than join() will take advantage of this information.\n",
    "\n",
    "For example, sortByKey() and groupByKey() will result in range-partitioned and hash-partitioned RDDs, respectively. \n",
    "\n",
    "On the other hand, operations like map() cause the new RDD to forget the parent’s partitioning information, because such operations could theoretically modify the key of each record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Example: PageRank Algorithm\n",
    "\n",
    "** from: *Learning Spark - O'Reilly* **\n",
    "\n",
    "As an example of a more involved algorithm that can benefit from RDD partitioning, we consider PageRank. \n",
    "\n",
    "The PageRank algorithm, named after Google’s Larry Page, aims to assign a measure of importance (a “rank”) to each document in a set based on how many documents have links to it.\n",
    "\n",
    "It can be used to rank web pages, of course, but also scientific articles, or influential users in a social network.\n",
    "\n",
    "PageRank is an iterative algorithm that performs many joins, so it is a good use case for RDD partitioning. \n",
    "\n",
    "The algorithm maintains two datasets: one of (pageID, link List) elements containing the list of neighbors of each page, and one of (pageID, rank) elements containing the current rank for each page. \n",
    "\n",
    "In our implementation we will use two datasets:\n",
    "   * A dataset containing papers (../data/countries_data.csv)\n",
    "   * A dataset containing the references of each paper (../data/countries_data_clean.csv)\n",
    "\n",
    "The Page Rank algorithm proceeds as follows:\n",
    "1. Initialize each page’s rank to 1.0.\n",
    "2. On each iteration, have page p send a contribution of rank(p)/numNeighbors(p) to its neighbors (the pages it has links to).\n",
    "3. Set each page’s rank to 0.15 + 0.85 * contributionsReceived\n",
    "\n",
    "The last two steps repeat for several iterations, during which the algorithm will converge\n",
    "to the correct PageRank value for each page. \n",
    "\n",
    "In practice, it’s typical to run about 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'1788', ([u'36956', u'5023', u'65053', u'14944', u'104357', u'107505', u'62679', u'26733', u'28602', u'16911', u'121268', u'77385', u'5406', u'51132', u'121026', u'99015', u'5872', u'54118', u'58266', u'44125', u'80108'], 1)), (u'13834', ([u'54799', u'72705', u'60562', u'97164', u'15494', u'6992', u'14100', u'112255', u'23192', u'21951', u'73127', u'34825', u'19154', u'64302', u'16479', u'82663', u'85660', u'12745', u'99806'], 1)), (u'11199', ([u'67234', u'29579', u'101522', u'110', u'21470'], 1)), (u'33928', ([u'58100', u'69677', u'79148', u'6155', u'112131', u'34467', u'18186', u'42629', u'87966', u'2983', u'115035', u'25221', u'117390', u'83158', u'16906', u'66112', u'112365', u'12644', u'115306', u'19512', u'12136', u'38764', u'23412', u'120334', u'102685', u'57538', u'46203', u'78161'], 1)), (u'4026', ([u'38375', u'51512', u'96056', u'105971', u'121452', u'46407', u'92839', u'60730', u'7723', u'119645'], 1)), (u'4372', ([u'87991', u'68001', u'75437', u'13938', u'78211', u'3382', u'92830', u'76902', u'67254', u'121078', u'24067', u'92637', u'22081', u'93212', u'69381', u'10213', u'12409', u'122009', u'71296', u'116536', u'55312', u'27705', u'69890'], 1)), (u'30235', ([u'72755', u'114428', u'122093', u'87529', u'93459', u'100788', u'102109', u'70190'], 1)), (u'8545', ([u'32382', u'103882', u'18054', u'51761', u'92299', u'43598', u'58617', u'43924', u'73058', u'24953', u'7957', u'12259', u'103673', u'92197', u'95038', u'99639', u'93897', u'73490', u'12426', u'10905'], 1)), (u'46331', ([u'22587', u'59770', u'30360', u'121859', u'93409', u'82405', u'122348', u'48480', u'32187', u'120675', u'1395', u'15077', u'60889'], 1)), (u'27367', ([u'40653', u'52610', u'59508', u'115278', u'101601', u'119346'], 1))]\n"
     ]
    }
   ],
   "source": [
    "def compute_contribs(refs, rank):\n",
    "    num_refs = len(refs)\n",
    "    contr = []\n",
    "    for ref in refs: \n",
    "        contr.append((ref, rank / float(num_refs)))\n",
    "    return contr\n",
    "\n",
    "        \n",
    "refs_rdd = sc.textFile(\"../data/papers_references.csv\").\\\n",
    "                    map(lambda x: (x.split(\",\")[0],x.split(\",\")[1].split(\";\"))).\\\n",
    "                    partitionBy(100).\\\n",
    "                    persist()\n",
    "refs_rdd = sc.parallelize(refs_rdd.take(500))\n",
    "#print refs_rdd.take(5)\n",
    "ranks_rdd = refs_rdd.mapValues(lambda x: 1)\n",
    "#print ranks_rdd.take(5)\n",
    "\n",
    "print refs_rdd.join(ranks_rdd).take(10)\n",
    "#for i in range(10):\n",
    "    contribs_rdd = refs_rdd.join(ranks_rdd).flatMap(\n",
    "                    lambda (ref, (refs, rank)): compute_contribs(refs, rank))\n",
    "    ranks_rdd = contribs_rdd.reduceByKey(lambda x,y: x+y).\\\n",
    "                    mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "#ranks_rdd.sortBy(lambda x: -x[1]).take(10)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
